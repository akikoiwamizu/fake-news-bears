{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0042c35e-7e52-4911-9233-c693c785ed21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.1)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2022.10.31-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (757 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m757.1/757.1 kB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0.0)\n",
      "Collecting huggingface-hub<1.0,>=0.11.0\n",
      "  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 kB\u001b[0m \u001b[31m123.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.14.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.4)\n",
      "Installing collected packages: regex, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.12.1 regex-2022.10.31 transformers-4.26.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1317d20-933b-4b68-a7bc-fd556e16b667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: google.cloud.bigquery in /opt/conda/lib/python3.7/site-packages (3.5.0)\n",
      "Requirement already satisfied: packaging>=20.0.0 in /opt/conda/lib/python3.7/site-packages (from google.cloud.bigquery) (23.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from google.cloud.bigquery) (2.28.2)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.7/site-packages (from google.cloud.bigquery) (2.8.2)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google.cloud.bigquery) (2.3.2)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /opt/conda/lib/python3.7/site-packages (from google.cloud.bigquery) (3.20.3)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.47.0 in /opt/conda/lib/python3.7/site-packages (from google.cloud.bigquery) (1.51.1)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google.cloud.bigquery) (2.4.1)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from google.cloud.bigquery) (1.22.2)\n",
      "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /opt/conda/lib/python3.7/site-packages (from google.cloud.bigquery) (1.34.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google.cloud.bigquery) (1.58.0)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google.cloud.bigquery) (2.16.1)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google.cloud.bigquery) (1.48.2)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google.cloud.bigquery) (1.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil<3.0dev,>=2.7.2->google.cloud.bigquery) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.21.0->google.cloud.bigquery) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.21.0->google.cloud.bigquery) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.21.0->google.cloud.bigquery) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.21.0->google.cloud.bigquery) (1.26.14)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google.cloud.bigquery) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google.cloud.bigquery) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google.cloud.bigquery) (0.2.8)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google.cloud.bigquery) (0.4.8)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: google.cloud.storage in /opt/conda/lib/python3.7/site-packages (2.7.0)\n",
      "Requirement already satisfied: google-resumable-media>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from google.cloud.storage) (2.4.1)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google.cloud.storage) (2.28.2)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /opt/conda/lib/python3.7/site-packages (from google.cloud.storage) (1.34.0)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /opt/conda/lib/python3.7/site-packages (from google.cloud.storage) (2.16.1)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /opt/conda/lib/python3.7/site-packages (from google.cloud.storage) (2.3.2)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5 in /opt/conda/lib/python3.7/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google.cloud.storage) (3.20.3)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google.cloud.storage) (1.58.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google.cloud.storage) (1.16.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google.cloud.storage) (5.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google.cloud.storage) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google.cloud.storage) (0.2.8)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media>=2.3.2->google.cloud.storage) (1.5.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google.cloud.storage) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google.cloud.storage) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google.cloud.storage) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google.cloud.storage) (2022.12.7)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google.cloud.storage) (0.4.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install google.cloud.bigquery\n",
    "!pip install google.cloud.storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41ed0bd5-7779-47a8-8432-39a7a2201ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com, https://download.pytorch.org/whl/cpu\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (1.13.1+cpu)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (0.14.1+cpu)\n",
      "Requirement already satisfied: torchaudio in /opt/conda/lib/python3.7/site-packages (0.13.1+cpu)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision) (1.21.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchvision) (2.28.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision) (3.4)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c03731ba-6065-4d13-8746-7005870caa14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: google-cloud-logging in ./.local/lib/python3.7/site-packages (3.5.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-logging) (2.3.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-logging) (1.22.2)\n",
      "Requirement already satisfied: google-cloud-appengine-logging<2.0.0dev,>=0.1.0 in ./.local/lib/python3.7/site-packages (from google-cloud-logging) (1.3.0)\n",
      "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.33.2 in /opt/conda/lib/python3.7/site-packages (from google-cloud-logging) (1.34.0)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /opt/conda/lib/python3.7/site-packages (from google-cloud-logging) (0.12.6)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /opt/conda/lib/python3.7/site-packages (from google-cloud-logging) (3.20.3)\n",
      "Requirement already satisfied: google-cloud-audit-log<1.0.0dev,>=0.1.0 in ./.local/lib/python3.7/site-packages (from google-cloud-logging) (0.2.5)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.33.2->google-cloud-logging) (2.16.1)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.33.2->google-cloud-logging) (2.28.2)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.33.2->google-cloud-logging) (1.58.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.33.2->google-cloud-logging) (1.51.1)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.33.2->google-cloud-logging) (1.48.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.33.2->google-cloud-logging) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.33.2->google-cloud-logging) (0.2.8)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.33.2->google-cloud-logging) (1.16.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.33.2->google-cloud-logging) (5.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.33.2->google-cloud-logging) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.33.2->google-cloud-logging) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.33.2->google-cloud-logging) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.33.2->google-cloud-logging) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.33.2->google-cloud-logging) (0.4.8)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: google-cloud-storage in /opt/conda/lib/python3.7/site-packages (2.7.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage) (2.28.2)\n",
      "Requirement already satisfied: google-resumable-media>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage) (2.4.1)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage) (2.3.2)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage) (2.16.1)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage) (1.34.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage) (1.58.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5 in /opt/conda/lib/python3.7/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage) (3.20.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (4.9)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (1.16.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (5.3.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media>=2.3.2->google-cloud-storage) (1.5.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage) (0.4.8)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install google-cloud-logging\n",
    "!pip3 install google-cloud-storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90c6b94d-3563-4785-a1c1-d9907b4a552f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import csv\n",
    "import urllib.request\n",
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import time \n",
    "import sys\n",
    "from typing import Union\n",
    "import google.cloud.aiplatform as vertex_ai\n",
    "from google.cloud import bigquery\n",
    "import google.cloud.logging_v2 as logging_v2\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "034c2313-30cc-4526-9acc-f3170c0866ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "bq_client = bigquery.Client(project=\"fake-news-bears\")\n",
    "storage_client = storage.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2dfc2e7d-67ce-45eb-a371-3a4a03d69543",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "client = logging_v2.client.Client()\n",
    "\n",
    "# set the format for the log\n",
    "google_log_format= logging.Formatter(\n",
    "fmt='%(name)s | %(module)s | %(funcName)s | %(message)s',\n",
    "datefmt='%Y-%m-$dT%H:%M:%S')\n",
    "\n",
    "\n",
    "handler = client.get_default_handler()\n",
    "handler.setFormatter(google_log_format)\n",
    "\n",
    "cloud_logger = logging.getLogger(\"scoring-model-logger\")\n",
    "cloud_logger.setLevel(\"INFO\")\n",
    "cloud_logger.addHandler(handler)\n",
    "\n",
    "log = logging.getLogger(\"vertex-ai-notebook-logger\")\n",
    "log.info(\"This is a log from the scoring model notebook\")\n",
    "log.info(\"Finished Downloading all required files. Now getting dataframe from BQ and loading libraries\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7570aca-8ce4-413f-9061-a380b630ed0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21fe401f-85aa-4993-a4d0-6c17136ab73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bq_query(sql: str) -> Union[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Input: SQL query, as a string, to execute in BigQuery\n",
    "    Returns the query results as a pandas DataFrame, or error, if any\n",
    "    \"\"\"\n",
    "\n",
    "    # Try dry run before executing query to catch any errors\n",
    "    job_config = bigquery.QueryJobConfig(dry_run=True, use_query_cache=False)\n",
    "    bq_client.query(sql, job_config=job_config)\n",
    "\n",
    "    # If dry run succeeds without errors, proceed to run query\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    client_result = bq_client.query(sql, job_config=job_config)\n",
    "\n",
    "    job_id = client_result.job_id\n",
    "\n",
    "    # Wait for query/job to finish running. then get & return data frame\n",
    "    df = client_result.result().to_arrow().to_pandas()\n",
    "    print(f\"Finished job_id: {job_id}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69185c64-2550-4925-be57-193516d3a721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished job_id: 7fee4441-cd21-4e18-b57d-46b9e8fbd417\n"
     ]
    }
   ],
   "source": [
    "sql_query = \"\"\"\n",
    "SELECT * FROM `fake-news-bears.usa_congress_twitter.tweets`\n",
    "WHERE text != \"\"\n",
    "\"\"\"\n",
    "\n",
    "df_input= run_bq_query(sql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d2ec7105-4cb1-4357-9395-1a883f5de5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We make a partition of the dataframe_tweets. We then modify part of it but its' not part of the parent dataframe so this error comes up. This is set to ignore that error since we append to list and it doesn't matter\n",
    "pd.options.mode.chained_assignment = None \n",
    "\n",
    "## Supress scientific notation for values\n",
    "pd.set_option('display.float_format', str)\n",
    "\n",
    "# Tasks:\n",
    "# emoji, emotion, hate, irony, offensive, sentiment\n",
    "# stance/abortion, stance/atheism, stance/climate, stance/feminist, stance/hillary\n",
    "\n",
    "## For now only doing 3 ML binary ml models: hate, irony and offensive. Will add more as we see fit.  \n",
    "potentialTasks = ['hate' ,'irony', 'offensive','all']\n",
    "defaultModel = \"cardiffnlp/twitter-roberta-base\" ## Choosing this model for this script. Will add other models in different scripts and import base functions from this script\n",
    "task = \"all\" ##which task are you looking for\n",
    "runtime = str(round(time.time(),0))[:-2]\n",
    "\n",
    "bucket_name = \"fake_news_bears_scoring_model\"\n",
    "folder = f'scoring_run_{runtime}/'\n",
    "bucket = storage_client.get_bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f702ab9-4698-49a4-bcb5-7ca09098955d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inputValidation(task):\n",
    "    if (task.lower() in potentialTasks):\n",
    "        log.info(f\"\\t Task is a valid choice. Moving on\")\n",
    "        task = task.lower()\n",
    "    else:\n",
    "        raise Exception(f\"\\t Task is not a valid Task. Choose one of the following: {potentialTasks}\")\n",
    "        sys.exit(1)\n",
    "    return task\n",
    "\n",
    "# Preprocess text (username and link placeholders)\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "def getTasks(task):\n",
    "    if task == \"hate\":\n",
    "        tasks = [\"hate\"]\n",
    "    elif task == \"offensive\":\n",
    "        tasks = [\"offensive\"]\n",
    "    elif task == [\"irony\"]:\n",
    "        tasks = \"irony\"\n",
    "    else:\n",
    "        tasks = [\"hate\",\"irony\",\"offensive\"]\n",
    "    return tasks\n",
    "## Will make this yargs later on for model directory\n",
    "## Default choice - twitter-roberta-base\n",
    "def get_Tokenizer(token_name,task):\n",
    "    model_dir = os.getcwd() + '/model'\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    TOKEN_repo = f\"{token_name}-{task}\"\n",
    "    roberta_model = model_dir + f'/{TOKEN_repo}'\n",
    "    tokenizer_config_file = roberta_model + '/tokenizer_config.json'\n",
    "\n",
    "    if Path(tokenizer_config_file).is_file():\n",
    "        log.info(f\"\\t Tokenizer File is ready. Loading Token File\")\n",
    "        log.debug(f\"\\t Token config file:{tokenizer_config_file}\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(roberta_model)\n",
    "    else:\n",
    "        log.info(f\"\\t Downloading Token files to {roberta_model} directory\")\n",
    "        log.debug(f\"\\t Model Token file:{tokenizer_config_file}\")\n",
    "        FILE_t_repo = f\"./model/{TOKEN_repo}\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(TOKEN_repo,force_download=True)\n",
    "        tokenizer.save_pretrained(FILE_t_repo) ##Choosing to save token files so that we can reuse when we dockerize and API this setup\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "def get_Model(model_name,task):\n",
    "    model_dir = os.getcwd() + '/model'\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    MODEL_repo = f\"{model_name}-{task}\"\n",
    "    roberta_model = model_dir + f'/{MODEL_repo}'\n",
    "    model_config_file = roberta_model + '/config.json'\n",
    "\n",
    "    if Path(model_config_file).is_file():\n",
    "        log.info(f\"\\t Model File is ready. Loading Model File\")\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(roberta_model)\n",
    "    else:\n",
    "        log.info(f\"\\t Downloading Model files to {roberta_model} directory\")\n",
    "        FILE_m_repo = f\"./model/{MODEL_repo}\"\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(MODEL_repo,force_download=True)\n",
    "        model.save_pretrained(FILE_m_repo) ##Choosing to save model files so that we can reuse when we dockerize and API this setup\n",
    "        \n",
    "    return model\n",
    "\n",
    "def get_Labels(task):\n",
    "    labels=[]\n",
    "    mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\n",
    "    with urllib.request.urlopen(mapping_link) as f:\n",
    "        html = f.read().decode('utf-8').split(\"\\n\")\n",
    "        csvreader = csv.reader(html, delimiter='\\t')\n",
    "    labels = [row[1] for row in csvreader if len(row) > 1]\n",
    "    log.debug(f\"\\t Labels are {labels}\")\n",
    "    return labels\n",
    "\n",
    "def writeListtoFile(fileList,directory):\n",
    "    log.debug(f\"\\t Writing data to file in {directory}\")\n",
    "    with open(rf\"{directory}\", 'w+') as fp:\n",
    "        for item in fileList:\n",
    "            fp.write(\"%s\\n\" % item)\n",
    "\n",
    "def readFiletoList(directory):\n",
    "    fileList = []\n",
    "    with open(rf\"{directory}\", 'r') as fp:\n",
    "        for line in fp:\n",
    "            item = line[:-1]\n",
    "            fileList.append(item)\n",
    "    return fileList\n",
    "\n",
    "def get_scoring_csvs(dataframe,task):\n",
    "    validateDataFrame(dataframe)\n",
    "    label = get_Labels(task)[0]\n",
    "    tokenizer = get_Tokenizer(defaultModel,task)\n",
    "    model = get_Model(defaultModel,task)\n",
    "    user_output_label = \"low_3_\" + str.replace(label,'-','_') + \"_tweets\"\n",
    "\n",
    "    saveDirectory = os.getcwd()+'/savestate'\n",
    "    os.makedirs(saveDirectory, exist_ok=True)\n",
    "    saveFile = saveDirectory + f\"/{task}_users.txt\"\n",
    "\n",
    "    if Path(saveFile).is_file():\n",
    "        log.info(f\"\\t Saved File exists from previous run. Continuing from Saved File in {saveFile}\")\n",
    "        userPartition = readFiletoList(saveFile)\n",
    "        totalUsers = len(userPartition)\n",
    "    else:\n",
    "        log.info(f\"\\t First time running job for {label}. Save file is in {saveFile}\")\n",
    "        userPartition = dataframe['author_id'].unique().tolist()\n",
    "        writeListtoFile(userPartition,saveFile)\n",
    "        totalUsers = len(userPartition)\n",
    "\n",
    "    users_directory = os.getcwd()+f'/users_score_{task}/'\n",
    "    tweets_directory = os.getcwd()+f'/tweets_score_{task}/'\n",
    "    log.debug(f\"\\t Making Users directory:{users_directory}\\n\\t Making Tweets directory:{tweets_directory}\")\n",
    "    os.makedirs(users_directory, exist_ok=True)\n",
    "    os.makedirs(tweets_directory, exist_ok=True)\n",
    "    user_final_list = []\n",
    "    tweet_final_list = []\n",
    "    time_list=[]\n",
    "    progress = 0\n",
    "\n",
    "    for user in userPartition:\n",
    "        user_scoring_list = []\n",
    "        tweet_scoring_list = []\n",
    "        userFile = users_directory + \"user_\" + str(user) + \"_\" + str.replace(label,'-','_') + \".csv\"\n",
    "        tweetFile = tweets_directory + \"tweets_\" + str(user) + \"_\" + str.replace(label,'-','_') + \".csv\"\n",
    "        startTime = time.time()\n",
    "        \n",
    "        dataframe_user=dataframe.loc[dataframe['author_id'] == user].astype(str) ##Prevent scientific notation\n",
    "        log.debug(f\"\\t There are {len(dataframe_user)} text rows to go through for user_id:{user}\")\n",
    "        encoded_series = dataframe_user['text'].apply(lambda x: tokenizer(x, return_tensors='pt'))\n",
    "        features = encoded_series.apply(lambda x: model(**x))\n",
    "        scores = features.apply(lambda x: x[0][0].detach().numpy())\n",
    "        scores_softmax = scores.apply(lambda x: softmax(x))\n",
    "        dataframe_user[label] = scores_softmax.apply(lambda x: x[0])\n",
    "        \n",
    "        score_value = round(dataframe_user[label].mean(),4) ##For now using mean but can change to median easily\n",
    "        top_3_btweets = dataframe_user[[label,'text']].sort_values(by=[label], ascending=True).head(3).values.tolist()\n",
    "        user_scoring_list.append([user,score_value,top_3_btweets])\n",
    "        tweet_scoring_list.extend(dataframe_user[['author_id','id',label]].values.tolist())\n",
    "        user_final_list.append([user,score_value,top_3_btweets])\n",
    "        tweet_final_list.extend(dataframe_user[['author_id','id',label]].values.tolist())\n",
    "        \n",
    "        user_DF = pd.DataFrame(user_scoring_list, columns = ['user_id',label,user_output_label])\n",
    "        tweet_DF = pd.DataFrame(tweet_scoring_list, columns = ['user_id','tweet_id',label]) \n",
    "        user_DF.to_csv(userFile, encoding='utf-8',index=False)\n",
    "        tweet_DF.to_csv(tweetFile,encoding='utf-8', index=False)\n",
    "\n",
    "        with open(saveFile, \"r\") as fp: ##Fixing Saved File in case it needs to restart. \n",
    "            lines = fp.readlines()\n",
    "\n",
    "        with open(saveFile, \"w\") as fp: ##Writing remaining user_ids back in \n",
    "            for line in lines:\n",
    "                if line.strip(\"\\n\") != str(user):\n",
    "                    fp.write(line)\n",
    "                else:\n",
    "                    log.debug(f\"\\t removing user {user}\")\n",
    "\n",
    "        endTime = time.time()\n",
    "        progress += 1\n",
    "        time_list.append([progress,round((endTime-startTime),3)])\n",
    "        log.debug(f\"\\t User Scoring List is \\n\\t {user_scoring_list}.  \\n\\t The size of the tweet scoring list is {len(user_scoring_list)}\")\n",
    "        log.debug(f\"\\t Tweet Scoring List is \\n\\t {tweet_scoring_list}.  \\n\\t The size of the tweet scoring list is {len(tweet_scoring_list)}\")\n",
    "        log.info(f\"\\t Finished {progress} users. {totalUsers-progress} users to go\")\n",
    "        log.debug(f\"\\t Duration of loop is {round((endTime-startTime),3)} seconds. Runs so far are {time_list}\")\n",
    "        time.sleep(2) ##Throttles CPU to make it manageable.  Encoding step is a ton of CPU cost and theirs no way around it.\n",
    "    tweets_final_DF = pd.DataFrame(tweet_final_list, columns = ['user_id','tweet_id',label])\n",
    "    users_final_DF = pd.DataFrame(user_final_list, columns = ['user_id',label,user_output_label])\n",
    "    return users_directory,tweets_directory,time_list,tweets_final_DF,users_final_DF\n",
    "\n",
    "def validateDataFrame(dataframe):\n",
    "    if isinstance(dataframe, pd.DataFrame):\n",
    "        if {'author_id', 'text'}.issubset(dataframe.columns):\n",
    "            dataframe['author_id'] = dataframe['author_id'].astype(str)\n",
    "            log.debug('\\t Dataframe Validated')\n",
    "        else:\n",
    "            raise Exception(\"Dataframe doesn't have [author_id] or [text] columns. Verify dataframe.columns exist and rename if necessary\")\n",
    "            sys.exit(1)\n",
    "    else:\n",
    "        raise Exception(\"Object is not DataFrame.  Please pass in valid DataFrame\")\n",
    "        sys.exit(1)\n",
    "        \n",
    "def csv_to_gcs(folder,task,directory):\n",
    "    final_folder = folder+task+\"/\"\n",
    "    folder_blob = bucket.blob(final_folder)\n",
    "    folder_blob.upload_from_string('')\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            try:\n",
    "                blob = bucket.blob(final_folder + filename)\n",
    "                blob.upload_from_filename(os.path.join(directory, filename))\n",
    "            except Exception as e:\n",
    "                print(f\"Error uploading {filename}: {str(e)}\")\n",
    "    return final_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee094c2-48b9-43f6-b477-ac459bd6cc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputValidation(task)\n",
    "tasks = getTasks(task)\n",
    "timeMetrics=[]\n",
    "for task in tasks:\n",
    "    directories=[]\n",
    "    users_directory,tweet_directory,time_list,tweets_final_DF,users_final_DF = get_scoring_csvs(df_input,task)\n",
    "    log.info(f\"\\t Finished {task}. {task} user files are in {users_directory} and {task} tweet files are in {tweet_directory}\")\n",
    "    directories.extend([users_directory])\n",
    "    directories.extend([tweet_directory])\n",
    "    timeMetrics.append([task,time_list])\n",
    "    log.info(f\"Uploading files to GCS\")\n",
    "    for directory in directories:\n",
    "        csv_to_gcs(folder,task,directory)\n",
    "        log.info(f\"Files have been uploaded to {bucket_name}\")\n",
    "    ## Add akiko's DF -> BQ function here\n",
    "log.info(f\"Finished All Tasks! Files are in {bucket_name} in folder {folder}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bd01c1-935f-4c23-92a5-c40755c890b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b517fd18-7d4c-4aad-82f2-4710a4630b4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d4a230-8fc3-4a54-946a-25d8f2f85f59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25bedee-af11-4a9d-aa4f-09de6782d300",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4ea4b-6f32-42a9-8eab-361e0b7c7349",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d968e9-a4ec-4bc7-b29a-c51f4eeb0be2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Local)",
   "language": "python",
   "name": "local-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
